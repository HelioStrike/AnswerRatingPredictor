{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from Vocab import *\n",
    "from model import AttentionModel\n",
    "\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "lr = 1\n",
    "embed_size = 256\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"It stands for Java Development Kit. It is the tool necessary to compile, document and package Java programs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = cleanText(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = getVocab('text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(embed_size, hidden_size, vocab_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sentence = sentence\n",
    "corr_tensor = torch.Tensor(vocab.getSentenceArray(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"It stands for Java Development Kit. It is the tool necessary to compile, document and package Java programs.\",\n",
    "                  \"It stands for Java Development Kit\",\n",
    "                  \"I don't know\",\n",
    "                  \"it is a good traversal\",\n",
    "                  \"And its a goal!!!\",\n",
    "                  \"It was developed by Java\"]\n",
    "test_sentences = [cleanText(s) for s in test_sentences]\n",
    "test_tensors = [torch.tensor(vocab.getSentenceArray(s)) for s in test_sentences]\n",
    "test_grades = [1, 0.7, 0.2, 0.1, 0, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krypt/myStuff/pytorch/tuts/1/AnswerRatingPredictor/model.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  corr_attn_params = F.softmax(self.lin_attn(corr).view(1, -1))\n",
      "/home/krypt/myStuff/pytorch/tuts/1/AnswerRatingPredictor/model.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  test_attn_params = F.softmax(self.lin_attn(test).view(1, -1))\n",
      "/home/krypt/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.7908607171848416\n",
      "Epoch: 1 Loss: 0.654195997864008\n",
      "Epoch: 2 Loss: 0.4944455726072192\n",
      "Epoch: 3 Loss: 0.2983965618768707\n",
      "Epoch: 4 Loss: 0.26732729584909976\n",
      "Epoch: 5 Loss: 0.20855257962830365\n",
      "Epoch: 6 Loss: 0.181807820099948\n",
      "Epoch: 7 Loss: 0.18797262955922633\n",
      "Epoch: 8 Loss: 0.11003358568996191\n",
      "Epoch: 9 Loss: 0.15872628315082693\n",
      "Epoch: 10 Loss: 0.11339493340346962\n",
      "Epoch: 11 Loss: 0.17005356433219276\n",
      "Epoch: 12 Loss: 0.08018149917097617\n",
      "Epoch: 13 Loss: 0.08818949795931985\n",
      "Epoch: 14 Loss: 0.0795739506429527\n",
      "Epoch: 15 Loss: 0.08080024803430774\n",
      "Epoch: 16 Loss: 0.08371634903596714\n",
      "Epoch: 17 Loss: 0.13056559237884358\n",
      "Epoch: 18 Loss: 0.08196555566973984\n",
      "Epoch: 19 Loss: 0.07044181019682583\n",
      "Epoch: 20 Loss: 0.06975771398720099\n",
      "Epoch: 21 Loss: 0.05973584286402911\n",
      "Epoch: 22 Loss: 0.08269635564647615\n",
      "Epoch: 23 Loss: 0.057123692822642624\n",
      "Epoch: 24 Loss: 0.06599061568704201\n",
      "Epoch: 25 Loss: 0.0449129528242338\n",
      "Epoch: 26 Loss: 0.05380491996766068\n",
      "Epoch: 27 Loss: 0.03405590419424698\n",
      "Epoch: 28 Loss: 0.039376331231324\n",
      "Epoch: 29 Loss: 0.023329456751525868\n",
      "Epoch: 30 Loss: 0.0246056233663694\n",
      "Epoch: 31 Loss: 0.01686356562822766\n",
      "Epoch: 32 Loss: 0.01617306128173368\n",
      "Epoch: 33 Loss: 0.01282752989573055\n",
      "Epoch: 34 Loss: 0.013008425514271948\n",
      "Epoch: 35 Loss: 0.013395500125625404\n",
      "Epoch: 36 Loss: 0.011548621245310642\n",
      "Epoch: 37 Loss: 0.014746900611498859\n",
      "Epoch: 38 Loss: 0.018330252249143086\n",
      "Epoch: 39 Loss: 0.013941149081801996\n",
      "Epoch: 40 Loss: 0.027372502750949934\n",
      "Epoch: 41 Loss: 0.03830646254209569\n",
      "Epoch: 42 Loss: 0.03753151845012326\n",
      "Epoch: 43 Loss: 0.032304643811585265\n",
      "Epoch: 44 Loss: 0.024042120945523493\n",
      "Epoch: 45 Loss: 0.012254349327122327\n",
      "Epoch: 46 Loss: 0.007429437431710539\n",
      "Epoch: 47 Loss: 0.0065340027649654076\n",
      "Epoch: 48 Loss: 0.0065459310899314005\n",
      "Epoch: 49 Loss: 0.005885279904077834\n"
     ]
    }
   ],
   "source": [
    "for e in range(50):\n",
    "    total_loss = 0\n",
    "    for t in range(len(test_tensors)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(corr_tensor.long(), test_tensors[t].long()).view(1)\n",
    "        y = torch.tensor(test_grades[t]).float().view(1)\n",
    "        loss = criterion(pred, y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", e, \"Loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krypt/myStuff/pytorch/tuts/1/AnswerRatingPredictor/model.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  corr_attn_params = F.softmax(self.lin_attn(corr).view(1, -1))\n",
      "/home/krypt/myStuff/pytorch/tuts/1/AnswerRatingPredictor/model.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  test_attn_params = F.softmax(self.lin_attn(test).view(1, -1))\n",
      "/home/krypt/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4436]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(corr_tensor.long(), test_tensors[5].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krypt/myStuff/pytorch/tuts/1/AnswerRatingPredictor/model.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  corr_attn_params = F.softmax(self.lin_attn(corr).view(1, -1))\n",
      "/home/krypt/myStuff/pytorch/tuts/1/AnswerRatingPredictor/model.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  test_attn_params = F.softmax(self.lin_attn(test).view(1, -1))\n",
      "/home/krypt/.local/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0826]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(corr_tensor.long(), torch.tensor(vocab.getSentenceArray(\"Its nice\")).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"saved_models/model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
